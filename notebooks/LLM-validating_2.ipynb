{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1078210d-b7c1-4878-9545-5f6cf210e6b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 11 22:36:01 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000               On  | 00000000:00:08.0 Off |                  Off |\n",
      "| 30%   34C    P8              18W / 230W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae60ee0c-1840-46a3-b8a9-3f9f06c01015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  javascript-common libexpat1-dev libjs-jquery libjs-sphinxdoc\n",
      "  libjs-underscore libpython3-dev libpython3.10 libpython3.10-dev\n",
      "  python3.10-dev\n",
      "Suggested packages:\n",
      "  apache2 | lighttpd | httpd\n",
      "The following NEW packages will be installed:\n",
      "  javascript-common libexpat1-dev libjs-jquery libjs-sphinxdoc\n",
      "  libjs-underscore libpython3-dev libpython3.10 libpython3.10-dev python3-dev\n",
      "  python3.10-dev\n",
      "0 upgraded, 10 newly installed, 0 to remove and 61 not upgraded.\n",
      "Need to get 7982 kB of archives.\n",
      "After this operation, 29.8 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 javascript-common all 11+nmu1 [5936 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libexpat1-dev amd64 2.4.7-1ubuntu0.3 [147 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjs-jquery all 3.6.0+dfsg+~3.5.13-1 [321 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjs-underscore all 1.13.2~dfsg-2 [118 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjs-sphinxdoc all 4.3.2-1 [139 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10 amd64 3.10.12-1~22.04.3 [1948 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-dev amd64 3.10.12-1~22.04.3 [4762 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3-dev amd64 3.10.6-1~22.04 [7166 B]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10-dev amd64 3.10.12-1~22.04.3 [507 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-dev amd64 3.10.6-1~22.04 [26.0 kB]\n",
      "Fetched 7982 kB in 1s (6543 kB/s)  \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package javascript-common.\n",
      "(Reading database ... 22469 files and directories currently installed.)\n",
      "Preparing to unpack .../0-javascript-common_11+nmu1_all.deb ...\n",
      "Unpacking javascript-common (11+nmu1) ...\n",
      "Selecting previously unselected package libexpat1-dev:amd64.\n",
      "Preparing to unpack .../1-libexpat1-dev_2.4.7-1ubuntu0.3_amd64.deb ...\n",
      "Unpacking libexpat1-dev:amd64 (2.4.7-1ubuntu0.3) ...\n",
      "Selecting previously unselected package libjs-jquery.\n",
      "Preparing to unpack .../2-libjs-jquery_3.6.0+dfsg+~3.5.13-1_all.deb ...\n",
      "Unpacking libjs-jquery (3.6.0+dfsg+~3.5.13-1) ...\n",
      "Selecting previously unselected package libjs-underscore.\n",
      "Preparing to unpack .../3-libjs-underscore_1.13.2~dfsg-2_all.deb ...\n",
      "Unpacking libjs-underscore (1.13.2~dfsg-2) ...\n",
      "Selecting previously unselected package libjs-sphinxdoc.\n",
      "Preparing to unpack .../4-libjs-sphinxdoc_4.3.2-1_all.deb ...\n",
      "Unpacking libjs-sphinxdoc (4.3.2-1) ...\n",
      "Selecting previously unselected package libpython3.10:amd64.\n",
      "Preparing to unpack .../5-libpython3.10_3.10.12-1~22.04.3_amd64.deb ...\n",
      "Unpacking libpython3.10:amd64 (3.10.12-1~22.04.3) ...\n",
      "Selecting previously unselected package libpython3.10-dev:amd64.\n",
      "Preparing to unpack .../6-libpython3.10-dev_3.10.12-1~22.04.3_amd64.deb ...\n",
      "Unpacking libpython3.10-dev:amd64 (3.10.12-1~22.04.3) ...\n",
      "Selecting previously unselected package libpython3-dev:amd64.\n",
      "Preparing to unpack .../7-libpython3-dev_3.10.6-1~22.04_amd64.deb ...\n",
      "Unpacking libpython3-dev:amd64 (3.10.6-1~22.04) ...\n",
      "Selecting previously unselected package python3.10-dev.\n",
      "Preparing to unpack .../8-python3.10-dev_3.10.12-1~22.04.3_amd64.deb ...\n",
      "Unpacking python3.10-dev (3.10.12-1~22.04.3) ...\n",
      "Selecting previously unselected package python3-dev.\n",
      "Preparing to unpack .../9-python3-dev_3.10.6-1~22.04_amd64.deb ...\n",
      "Unpacking python3-dev (3.10.6-1~22.04) ...\n",
      "Setting up javascript-common (11+nmu1) ...\n",
      "Setting up libpython3.10:amd64 (3.10.12-1~22.04.3) ...\n",
      "Setting up libexpat1-dev:amd64 (2.4.7-1ubuntu0.3) ...\n",
      "Setting up libpython3.10-dev:amd64 (3.10.12-1~22.04.3) ...\n",
      "Setting up python3.10-dev (3.10.12-1~22.04.3) ...\n",
      "Setting up libjs-jquery (3.6.0+dfsg+~3.5.13-1) ...\n",
      "Setting up libjs-underscore (1.13.2~dfsg-2) ...\n",
      "Setting up libpython3-dev:amd64 (3.10.6-1~22.04) ...\n",
      "Setting up libjs-sphinxdoc (4.3.2-1) ...\n",
      "Setting up python3-dev (3.10.6-1~22.04) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install python3-dev -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc392a7-7b1e-415c-845f-e8cb2c6c0086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-v9dnc7dr/unsloth_2d7b4e65baec442c9ffc968774e8df5e\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-v9dnc7dr/unsloth_2d7b4e65baec442c9ffc968774e8df5e\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 4211cc01409e3ced4f7abebaf68e244193b46e2c\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tyro (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting transformers>=4.38.2 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.16.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.65.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.0)\n",
      "Collecting wheel>=0.42.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.3)\n",
      "Collecting protobuf<4.0.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.1)\n",
      "Collecting pyarrow>=12.0.0 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.31.0)\n",
      "Collecting xxhash (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.12.2)\n",
      "Collecting aiohttp (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading regex-2024.5.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19 (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.9.0)\n",
      "Collecting rich>=11.1.0 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.11.17)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.15.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.1/774.1 kB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.4-py3-none-any.whl size=101150 sha256=abc579ed5bc30809a3424471bf4e4ba2f273fbe857af1e454e3c68065e030b57\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pjoiotya/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
      "Successfully built unsloth\n",
      "Installing collected packages: sentencepiece, xxhash, wheel, unsloth, tzdata, shtab, safetensors, regex, pyarrow-hotfix, pyarrow, protobuf, multidict, mdurl, frozenlist, docstring-parser, dill, async-timeout, yarl, pandas, multiprocess, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, aiohttp, tyro, transformers, datasets\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.41.2\n",
      "    Uninstalling wheel-0.41.2:\n",
      "      Successfully uninstalled wheel-0.41.2\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.19.1 dill-0.3.8 docstring-parser-0.16 frozenlist-1.4.1 huggingface-hub-0.23.0 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.2 protobuf-3.20.3 pyarrow-16.0.0 pyarrow-hotfix-0.6 regex-2024.5.10 rich-13.7.1 safetensors-0.4.3 sentencepiece-0.2.0 shtab-1.7.1 tokenizers-0.19.1 transformers-4.40.2 tyro-0.8.4 tzdata-2024.1 unsloth-2024.4 wheel-0.43.0 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0f84c1-81e9-4422-86df-f7f1012c27a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers<0.0.26\n",
      "  Downloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Downloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, xformers, trl, peft, accelerate\n",
      "Successfully installed accelerate-0.30.1 bitsandbytes-0.43.1 peft-0.10.0 trl-0.8.6 xformers-0.0.25.post1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd9a9f1-193c-4025-aa03-8b0e958bb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365f053f-a302-49b1-845c-3332716914d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You passed in `unsloth/mistral-7b-bnb-4bit` which is a 4bit model, yet you set\n",
      "`load_in_4bit = False`. We shall load `unsloth/mistral-7b` instead.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.679 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830dd37f6a7e4b108bb016fb1c469429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b178d29-454d-4783-a399-0868e3661d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 128,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 1280,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14ce47e5-394e-4fa3-b412-12e80cb762d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makecode(prompt):\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "    prompt\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True, temperature = 0)\n",
    "    output = tokenizer.batch_decode(outputs)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d093b955-7639-4504-b775-80dee706cb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++\n",
      "prompt1_truthtable.v\n",
      "Vgen/intermediate8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate8_1\n",
      "++++++++++\n",
      "prompt2_truthtable.v\n",
      "Vgen/intermediate8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate8_2\n",
      "++++++++++\n",
      "prompt3_truthtable.v\n",
      "Vgen/intermediate8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate8_3\n",
      "++++++++++\n",
      "prompt1_shift-left-rotate.v\n",
      "Vgen/intermediate5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate5_1\n",
      "++++++++++\n",
      "prompt2_shift-left-rotate.v\n",
      "Vgen/intermediate5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate5_2\n",
      "++++++++++\n",
      "prompt3_shift-left-rotate.v\n",
      "Vgen/intermediate5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate5_3\n",
      "++++++++++\n",
      "prompt1_permutation.v\n",
      "Vgen/intermediate7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate7_1\n",
      "++++++++++\n",
      "prompt2_permutation.v\n",
      "Vgen/intermediate7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate7_2\n",
      "++++++++++\n",
      "prompt3_permutation.v\n",
      "Vgen/intermediate7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate7_3\n",
      "++++++++++\n",
      "prompt1_lfsr.v\n",
      "Vgen/intermediate3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate3_1\n",
      "++++++++++\n",
      "prompt2_lfsr.v\n",
      "Vgen/intermediate3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate3_2\n",
      "++++++++++\n",
      "prompt3_lfsr.v\n",
      "Vgen/intermediate3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate3_3\n",
      "++++++++++\n",
      "prompt1_ram.v\n",
      "Vgen/intermediate6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate6_1\n",
      "++++++++++\n",
      "prompt2_ram.v\n",
      "Vgen/intermediate6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate6_2\n",
      "++++++++++\n",
      "prompt3_ram.v\n",
      "Vgen/intermediate6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate6_3\n",
      "++++++++++\n",
      "prompt1_simple-fsm.v\n",
      "Vgen/intermediate4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate4_1\n",
      "++++++++++\n",
      "prompt2_simple-fsm.v\n",
      "Vgen/intermediate4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate4_2\n",
      "++++++++++\n",
      "prompt3_simple-fsm.v\n",
      "Vgen/intermediate4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate4_3\n",
      "++++++++++\n",
      "prompt1_half_adder.v\n",
      "Vgen/intermediate1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate1_1\n",
      "++++++++++\n",
      "prompt2_half_adder.v\n",
      "Vgen/intermediate1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate1_2\n",
      "++++++++++\n",
      "prompt3_half_adder.v\n",
      "Vgen/intermediate1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate1_3\n",
      "++++++++++\n",
      "prompt1_priority_encoder.v\n",
      "Vgen/basic3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic3_1\n",
      "++++++++++\n",
      "prompt2_priority_encoder.v\n",
      "Vgen/basic3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic3_2\n",
      "++++++++++\n",
      "prompt3_priority_encoder.v\n",
      "Vgen/basic3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic3_3\n",
      "++++++++++\n",
      "prompt1_counter.v\n",
      "Vgen/intermediate2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate2_1\n",
      "++++++++++\n",
      "prompt2_counter.v\n",
      "Vgen/intermediate2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate2_2\n",
      "++++++++++\n",
      "prompt3_counter.v\n",
      "Vgen/intermediate2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate2_3\n",
      "++++++++++\n",
      "prompt1_mux.v\n",
      "Vgen/basic4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic4_1\n",
      "++++++++++\n",
      "prompt2_mux.v\n",
      "Vgen/basic4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic4_2\n",
      "++++++++++\n",
      "prompt3_mux.v\n",
      "Vgen/basic4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic4_3\n",
      "++++++++++\n",
      "prompt1_and_gate.v\n",
      "Vgen/basic2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic2_1\n",
      "++++++++++\n",
      "prompt2_and_gate.v\n",
      "Vgen/basic2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic2_2\n",
      "++++++++++\n",
      "prompt3_and_gate.v\n",
      "Vgen/basic2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic2_3\n",
      "++++++++++\n",
      "prompt1_wire_assign.v\n",
      "Vgen/basic1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic1_1\n",
      "++++++++++\n",
      "prompt2_wire_assign.v\n",
      "Vgen/basic1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic1_2\n",
      "++++++++++\n",
      "prompt3_wire_assign.v\n",
      "Vgen/basic1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic1_3\n",
      "++++++++++\n",
      "prompt1_abro.v\n",
      "Vgen/advanced5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced5_1\n",
      "++++++++++\n",
      "prompt2_abro.v\n",
      "Vgen/advanced5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced5_2\n",
      "++++++++++\n",
      "prompt3_abro.v\n",
      "Vgen/advanced5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced5_3\n",
      "++++++++++\n",
      "prompt1_advshifter.v\n",
      "Vgen/advanced4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced4_1\n",
      "++++++++++\n",
      "prompt2_advshifter.v\n",
      "Vgen/advanced4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced4_2\n",
      "++++++++++\n",
      "prompt3_advshifter.v\n",
      "Vgen/advanced4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced4_3\n",
      "++++++++++\n",
      "prompt1_advfsm.v\n",
      "Vgen/advanced3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced3_1\n",
      "++++++++++\n",
      "prompt2_advfsm.v\n",
      "Vgen/advanced3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced3_2\n",
      "++++++++++\n",
      "prompt3_advfsm.v\n",
      "Vgen/advanced3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced3_3\n",
      "++++++++++\n",
      "prompt1_countslow.v\n",
      "Vgen/advanced2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced2_1\n",
      "++++++++++\n",
      "prompt2_countslow.v\n",
      "Vgen/advanced2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced2_2\n",
      "++++++++++\n",
      "prompt3_countslow.v\n",
      "Vgen/advanced2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced2_3\n",
      "++++++++++\n",
      "prompt1_signed-addition-overflow.v\n",
      "Vgen/advanced1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced1_1\n",
      "++++++++++\n",
      "prompt2_signed-addition-overflow.v\n",
      "Vgen/advanced1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced1_2\n",
      "++++++++++\n",
      "prompt3_signed-addition-overflow.v\n",
      "Vgen/advanced1\n",
      "advanced1_3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Задаем корневую директорию\n",
    "root_dir = 'Vgen'\n",
    "content = ''\n",
    "\n",
    "# Проходим по всем директориям и файлам в корневой директории\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    num = 0\n",
    "    for file in files:\n",
    "        # Проверяем, содержит ли имя файла 'prompt'\n",
    "        if 'prompt' in file:\n",
    "            \n",
    "            print('++++++++++')\n",
    "            print(file)\n",
    "            print(subdir)\n",
    "            if subdir == 'Vgen/basic1/.ipynb_checkpoints':\n",
    "                continue\n",
    "            \n",
    "            num = num + 1\n",
    "            # Формируем полный путь к файлу\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            #print(file)\n",
    "            # Читаем содержимое файла и добавляем его в переменную content\n",
    "            #print(file_path)\n",
    "            #print(file_path.replace('Vgen/', \"\").replace('/'+file,''))\n",
    "            #vfile = file_path.replace('Vgen/', \"\").replace('/'+file,'')\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "            output = makecode(content)\n",
    "            print((subdir[5:]+'_'+str(num)).strip('/'))\n",
    "            with open(subdir[5:]+'_'+str(num), 'w') as file:\n",
    "                file.write(output[0])\n",
    "                \n",
    "\n",
    "# Указываем путь и имя файла, в который будет сохранен результат\n",
    "output_file_path = 'output.txt'\n",
    "# Записываем содержимое в файл\n",
    "#with open(output_file_path, 'w') as output_file:\n",
    "#    output_file.write(content)\n",
    "\n",
    "#print(f\"Содержимое сохранено в файл: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b079ce45-3e8c-40bf-807f-f50cf985fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"I want you to act as an IC designer, and implement the following in Verilog.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"description\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1b6b2dc-df81-4798-8ef5-f24349171eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s> \\n// This is a 3-bit priority encoder. It outputs the position of the first high bit.\\nmodule priority_encoder( \\ninput [2:0] in,\\noutput reg [1:0] pos ); \\n// If in==0, assign zero to pos\\n// If in[0] is high, assign 0 to pos\\n// If in[1] is high, assign 1 to pos\\n// If in[2] is high, assign 2 to pos\\nalways @(in) begin\\nif (in==0) pos <= 0;\\nelse if (in[0]) pos <= 0;\\nelse if (in[1]) pos <= 1;\\nelse if (in[2]) pos <= 2;\\nend\\nendmodule\\n\\n// This is a 3-bit priority decoder. It outputs the position of the first high bit.\\nmodule priority_decoder( \\ninput [1:0] pos,\\noutput reg [2:0] out ); \\n// If pos==0, assign zero to out\\n// If pos==1, assign 1 to out\\n// If pos==2, assign 2 to out\\nalways @(pos) begin\\nif (pos==0) out <= 0;\\nelse if (pos==1) out <= 1;\\nelse if (pos==2) out <= 2;\\nend\\nendmodule\\n\\n// This is a 3-bit priority encoder. It outputs the position of the first high bit.\\nmodule priority_encoder( \\ninput [2:0] in,\\noutput reg [1:0] pos ); \\n// If in==0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "\"\"\"\n",
    "// This is a 3-bit priority encoder. It outputs the position of the first high bit.\n",
    "module priority_encoder( \n",
    "input [2:0] in,\n",
    "output reg [1:0] pos ); \n",
    "// If in==0, assign zero to pos\n",
    "// If in[0] is high, assign 0 to pos\n",
    "// If in[1] is high, assign 1 to pos\n",
    "// If in[2] is high, assign 2 to pos\n",
    "\"\"\"\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True, temperature = 0.3)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "295270de-9ec4-46ec-8e7b-408d999b1a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model_80steps_60k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53c323fb-0e68-4b66-9304-a791d25fc505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model_100k_100step/tokenizer_config.json',\n",
       " 'lora_model_100k_100step/special_tokens_map.json',\n",
       " 'lora_model_100k_100step/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model_60k_80steps\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model_60k_80steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "446070a9-516e-4dd9-a62f-6396ba7ab74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4e86f1b2a0485f8f73081073d7c6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ada2bd6df974716a1943373f6ebcdb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/Driseri/lora_model_llama3-8B-Verigen\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\"Driseri/lora_model_llama3-8B-Verigen\", token = \"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\") # Online saving\n",
    "tokenizer.push_to_hub(\"Driseri/lora_model_llama3-8B-Verigen\", token = \"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\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583682c-1730-475e-9c96-77bb44cd7553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
